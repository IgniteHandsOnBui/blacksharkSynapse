{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "innosyn-pipeline-syn-ws"
		},
		"LiamTest_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'LiamTest'"
		},
		"Stellar Airbus v2 FS_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'Stellar Airbus v2 FS'"
		},
		"Stellar Data Storage Account v2_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'Stellar Data Storage Account v2'"
		},
		"innoaoaidata_STAC_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'innoaoaidata_STAC'"
		},
		"innosyn-pipeline-syn-ws-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'innosyn-pipeline-syn-ws-WorkspaceDefaultSqlServer'"
		},
		"Stellar Airbus v2_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://rawdatajbqezt.dfs.core.windows.net"
		},
		"Stellar Pipeline Key Vault_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://innosyn-pipeline-kv.vault.azure.net/"
		},
		"innosyn-pipeline-syn-ws-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://synhnsdl2xmc.dfs.core.windows.net/"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Blackshark Building Detection')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Wait for Blackshark Model",
						"type": "Until",
						"dependsOn": [
							{
								"activity": "Blackshark Model",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@and(and(not(equals(string(variables('FunctionCompleted')), 'running')), not(equals(string(variables('FunctionCompleted')), 'active'))), not(equals(string(variables('FunctionCompleted')), 'preparing')))",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "Wait for Blackshark",
									"type": "Wait",
									"dependsOn": [
										{
											"activity": "Set FunctionCompleted Custom Vision",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"waitTimeInSeconds": 30
									}
								},
								{
									"name": "Check Status Blackshark",
									"type": "WebActivity",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"url": {
											"value": "@concat('https://',pipeline().parameters.BatchName,'.',pipeline().parameters.BatchLocation,'.batch.azure.com/jobs/',pipeline().parameters.JobName,'/tasks/stllr-bshrk-task-', pipeline().RunId, '?api-version=2022-01-01.15.0')",
											"type": "Expression"
										},
										"connectVia": {
											"referenceName": "AutoResolveIntegrationRuntime",
											"type": "IntegrationRuntimeReference"
										},
										"method": "GET",
										"headers": {},
										"authentication": {
											"type": "MSI",
											"resource": "https://batch.core.windows.net/"
										}
									}
								},
								{
									"name": "Set FunctionCompleted Custom Vision",
									"type": "SetVariable",
									"dependsOn": [
										{
											"activity": "Check Status Blackshark",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"variableName": "FunctionCompleted",
										"value": {
											"value": "@activity('Check Status Blackshark').output['state']",
											"type": "Expression"
										}
									}
								},
								{
									"name": "Set FunctionError",
									"type": "SetVariable",
									"dependsOn": [
										{
											"activity": "Check Status Blackshark",
											"dependencyConditions": [
												"Failed"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"variableName": "FunctionError",
										"value": {
											"value": "@activity('Check Status Blackshark').output['executionInfo']['failureInfo']",
											"type": "Expression"
										}
									}
								}
							],
							"timeout": "7.00:00:00"
						}
					},
					{
						"name": "Blackshark Model",
						"type": "WebActivity",
						"dependsOn": [
							{
								"activity": "Copy Config file",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@concat('https://',pipeline().parameters.BatchName,'.',pipeline().parameters.BatchLocation,'.batch.azure.com/jobs/',pipeline().parameters.JobName,'/tasks?api-version=2020-03-01.11.0')",
								"type": "Expression"
							},
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "POST",
							"headers": {
								"Content-type": "application/json; odata=minimalmetadata; charset=utf-8"
							},
							"body": {
								"value": "@json(concat('{\n  \"id\": \"stllr-bshrk-task-', pipeline().RunId, '\",\n  \"commandLine\": \"\",\n  \"containerSettings\": {\n    \"imageName\": \"', activity('Read Spec Document').output['runStatus'].output.sink.value[0]['algImageName'],'\",\n    \"containerRunOptions\": \"--rm --workdir / -v /mnt/batch/tasks/fsmounts/S/', pipeline().parameters.Prefix, ':', activity('Read Spec Document').output['runStatus'].output.sink.value[0]['mountedDirectory'], '\",\n    \"registry\": {\n        \"registryServer\": \"', split(activity('Read Spec Document').output['runStatus'].output.sink.value[0]['algImageName'], '/')[0], '\",\n        \"username\": \"external-msft\",\n        \"password\": \"9STIVwl2RiaQqDABXghQa7Xtliwz=JoY\"\n    }\n  },\n  \"userIdentity\": {\n        \"autoUser\": {\n            \"scope\": \"pool\",\n            \"elevationLevel\": \"admin\"\n        }\n    }\n}'))",
								"type": "Expression"
							},
							"authentication": {
								"type": "MSI",
								"resource": "https://batch.core.windows.net/"
							}
						}
					},
					{
						"name": "Copy GeoTiff",
						"type": "SparkJob",
						"dependsOn": [
							{
								"activity": "Read Spec Document",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"sparkJob": {
								"referenceName": "Copy noop",
								"type": "SparkJobDefinitionReference"
							},
							"file": "abfss://spark-jobs@synhnsdl2xmc.dfs.core.windows.net/copy_noop/src/main.py",
							"args": [
								"--storage_account_name",
								"@pipeline().parameters.StorageAccountName",
								"--src_container",
								"@pipeline().parameters.Prefix",
								"--src_folder",
								"warp",
								"--storage_account_key",
								"@pipeline().parameters.StorageAccountKey",
								"--dst_fileshare",
								"volume-a",
								"--dst_folder",
								"@concat(pipeline().parameters.Prefix,'/', activity('Read Spec Document').output['runStatus'].output.sink.value[0]['submissionDirectory'])",
								"--folders_to_create",
								"@concat(pipeline().parameters.Prefix,'/', activity('Read Spec Document').output['runStatus'].output.sink.value[0]['mountedDirectory'])",
								"--folders_to_create",
								"@concat(pipeline().parameters.Prefix,'/', activity('Read Spec Document').output['runStatus'].output.sink.value[0]['resultsDirectory'])",
								"--folders_to_create",
								"@concat(pipeline().parameters.Prefix,'/', activity('Read Spec Document').output['runStatus'].output.sink.value[0]['logsDirectory'])"
							],
							"targetBigDataPool": {
								"referenceName": "pooldl2xmc",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Medium",
							"conf": {
								"spark.dynamicAllocation.minExecutors": 2,
								"spark.dynamicAllocation.maxExecutors": 3
							},
							"driverSize": "Medium",
							"numExecutors": 2
						}
					},
					{
						"name": "Copy Config file",
						"type": "SparkJob",
						"dependsOn": [
							{
								"activity": "Copy GeoTiff",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"sparkJob": {
								"referenceName": "Copy noop",
								"type": "SparkJobDefinitionReference"
							},
							"file": "abfss://spark-jobs@synhnsdl2xmc.dfs.core.windows.net/copy_noop/src/main.py",
							"args": [
								"--storage_account_name",
								"@pipeline().parameters.StorageAccountName",
								"--src_container",
								"@pipeline().parameters.Prefix",
								"--src_folder",
								"@concat('config/', activity('Read Spec Document').output['runStatus'].output.sink.value[0]['contextFileName'])",
								"--storage_account_key",
								"@pipeline().parameters.StorageAccountKey",
								"--dst_fileshare",
								"volume-a",
								"--dst_folder",
								"@concat(pipeline().parameters.Prefix,'/', activity('Read Spec Document').output['runStatus'].output.sink.value[0]['contextFileName'])"
							],
							"targetBigDataPool": {
								"referenceName": "pooldl2xmc",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Medium",
							"conf": {
								"spark.dynamicAllocation.minExecutors": 2,
								"spark.dynamicAllocation.maxExecutors": 3
							},
							"driverSize": "Medium",
							"numExecutors": 2
						}
					},
					{
						"name": "Copy Raster and Vector Output",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Wait for Blackshark Model",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "BinarySource",
								"storeSettings": {
									"type": "AzureFileStorageReadSettings",
									"recursive": true,
									"wildcardFolderPath": {
										"value": "@concat(pipeline().parameters.Prefix, '/', activity('Read Spec Document').output['runStatus'].output.sink.value[0]['resultsDirectory'])",
										"type": "Expression"
									},
									"wildcardFileName": "*.*",
									"deleteFilesAfterCompletion": false
								},
								"formatSettings": {
									"type": "BinaryReadSettings"
								}
							},
							"sink": {
								"type": "BinarySink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "gls",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "gld",
								"type": "DatasetReference",
								"parameters": {
									"DestinationFolderPath": "detections",
									"DestinationContainerName": {
										"value": "@pipeline().parameters.Prefix",
										"type": "Expression"
									}
								}
							}
						]
					},
					{
						"name": "Read Spec Document",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "ReadSpecDocumentFlow",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source": {
										"filename": "bshrk-building.json",
										"folderpath": "config",
										"containername": {
											"value": "@pipeline().parameters.Prefix",
											"type": "Expression"
										}
									},
									"sink": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "None",
							"cacheSinks": {
								"firstRowOnly": true
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"Prefix": {
						"type": "string",
						"defaultValue": "innoblackshark"
					},
					"BatchName": {
						"type": "string",
						"defaultValue": "blackshark-model"
					},
					"BatchLocation": {
						"type": "string",
						"defaultValue": "southcentralus"
					},
					"JobName": {
						"type": "string",
						"defaultValue": "blackshark-model"
					},
					"StorageAccountName": {
						"type": "string",
						"defaultValue": "rawdatajbqezt"
					},
					"StorageAccountKey": {
						"type": "string",
						"defaultValue": "AHOTVR7kOK3lLcJV177VGv8WaPpTd7GfYdds2XFWD7gLDD89bIzAbMj47eeLWCS+igKPwum0Ip1lCVpEUdtIrA=="
					}
				},
				"variables": {
					"FunctionCompleted": {
						"type": "String"
					},
					"FunctionError": {
						"type": "String"
					}
				},
				"annotations": [],
				"lastPublishTime": "2022-05-31T20:09:05Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/sparkJobDefinitions/Copy noop')]",
				"[concat(variables('workspaceId'), '/bigDataPools/pooldl2xmc')]",
				"[concat(variables('workspaceId'), '/datasets/gls')]",
				"[concat(variables('workspaceId'), '/datasets/gld')]",
				"[concat(variables('workspaceId'), '/dataflows/ReadSpecDocumentFlow')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Blackshark Model Transforms')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Warp to change CRS",
						"type": "SparkJob",
						"dependsOn": [
							{
								"activity": "Crop",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"sparkJob": {
								"referenceName": "Warp",
								"type": "SparkJobDefinitionReference"
							},
							"file": "abfss://spark-jobs@synhnsdl2xmc.dfs.core.windows.net/raster_warp/src/warp.py",
							"args": [
								"--storage_account_name",
								"@pipeline().parameters.StorageAccountName",
								"--storage_account_key",
								"@pipeline().parameters.StorageAccountKey",
								"--storage_container",
								"@pipeline().parameters.Prefix",
								"--src_folder_name",
								"crop"
							],
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"numExecutors": null
						}
					},
					{
						"name": "Crop",
						"type": "SparkJob",
						"dependsOn": [
							{
								"activity": "More than one GeoTiff",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"sparkJob": {
								"referenceName": "Crop",
								"type": "SparkJobDefinitionReference"
							},
							"file": "abfss://spark-jobs@synhnsdl2xmc.dfs.core.windows.net/raster_crop/src/crop.py",
							"args": [
								"--storage_account_name",
								"@pipeline().parameters.StorageAccountName",
								"--storage_account_key",
								"@pipeline().parameters.StorageAccountKey",
								"--storage_container",
								"@pipeline().parameters.Prefix",
								"--src_folder_name",
								"@variables('CropSourceFolder')",
								"--config_file_name",
								"config-aoi.json",
								"--linked_service_name",
								"Stellar Airbus v2"
							],
							"targetBigDataPool": {
								"referenceName": "pooldl2xmc",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Medium",
							"conf": {
								"spark.dynamicAllocation.minExecutors": 2,
								"spark.dynamicAllocation.maxExecutors": 3
							},
							"driverSize": "Medium",
							"numExecutors": 2
						}
					},
					{
						"name": "GetFilesToMosaic",
						"type": "GetMetadata",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "rawtifs",
								"type": "DatasetReference",
								"parameters": {
									"containername": {
										"value": "@pipeline().parameters.Prefix",
										"type": "Expression"
									},
									"folderpath": "raw"
								}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobStorageReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "BinaryReadSettings"
							}
						}
					},
					{
						"name": "More than one GeoTiff",
						"type": "IfCondition",
						"dependsOn": [
							{
								"activity": "For Each File to Mosaic",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@greater(length(activity('GetFilesToMosaic').output.childItems),1)",
								"type": "Expression"
							},
							"ifFalseActivities": [
								{
									"name": "Set Crop Source Folder to raw",
									"type": "SetVariable",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"variableName": "CropSourceFolder",
										"value": "raw"
									}
								}
							],
							"ifTrueActivities": [
								{
									"name": "Mosaic",
									"type": "SparkJob",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"sparkJob": {
											"referenceName": "Mosaic",
											"type": "SparkJobDefinitionReference"
										},
										"file": "abfss://spark-jobs@synhnsdl2xmc.dfs.core.windows.net/raster_mosaic/src/mosaic.py",
										"args": [
											"--storage_account_name",
											"@pipeline().parameters.StorageAccountName",
											"--storage_account_key",
											"@pipeline().parameters.StorageAccountKey",
											"--storage_container",
											"@pipeline().parameters.Prefix",
											"--src_folder_name",
											"raw"
										],
										"targetBigDataPool": {
											"referenceName": "pooldl2xmc",
											"type": "BigDataPoolReference"
										},
										"executorSize": "Medium",
										"conf": {
											"spark.dynamicAllocation.minExecutors": 2,
											"spark.dynamicAllocation.maxExecutors": 3
										},
										"driverSize": "Medium",
										"numExecutors": 2
									}
								},
								{
									"name": "Set Crop Source Folder to mosaic",
									"type": "SetVariable",
									"dependsOn": [
										{
											"activity": "Mosaic",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"variableName": "CropSourceFolder",
										"value": "mosaic"
									}
								}
							]
						}
					},
					{
						"name": "For Each File to Mosaic",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "GetFilesToMosaic",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('GetFilesToMosaic').output.childItems",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "Set Mosaic File Names",
									"type": "SetVariable",
									"dependsOn": [
										{
											"activity": "Store Temp Mosaic File Names",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"variableName": "MosaicFileNames",
										"value": {
											"value": "@concat(variables('TempMosaicFileNames'), if(equals(variables('TempMosaicFileNames'), ''),'',','), item().name)",
											"type": "Expression"
										}
									}
								},
								{
									"name": "Store Temp Mosaic File Names",
									"type": "SetVariable",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"variableName": "TempMosaicFileNames",
										"value": {
											"value": "@variables('MosaicFileNames')",
											"type": "Expression"
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"Prefix": {
						"type": "string"
					},
					"StorageAccountName": {
						"type": "string"
					},
					"StorageAccountKey": {
						"type": "string"
					}
				},
				"variables": {
					"FunctionCompleted": {
						"type": "String",
						"defaultValue": "None"
					},
					"FunctionError": {
						"type": "String"
					},
					"MosaicFileNames": {
						"type": "String"
					},
					"TempMosaicFileNames": {
						"type": "String"
					},
					"CropSourceFolder": {
						"type": "String"
					}
				},
				"annotations": [],
				"lastPublishTime": "2022-05-31T15:14:13Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sparkJobDefinitions/Warp')]",
				"[concat(variables('workspaceId'), '/sparkJobDefinitions/Crop')]",
				"[concat(variables('workspaceId'), '/bigDataPools/pooldl2xmc')]",
				"[concat(variables('workspaceId'), '/datasets/rawtifs')]",
				"[concat(variables('workspaceId'), '/sparkJobDefinitions/Mosaic')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/E2E Blackshark Model Flow v2')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Blackshark Model Transforms",
						"type": "ExecutePipeline",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Blackshark Model Transforms",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"Prefix": {
									"value": "@pipeline().parameters.Prefix",
									"type": "Expression"
								},
								"StorageAccountName": {
									"value": "@pipeline().parameters.StorageAccountName",
									"type": "Expression"
								},
								"StorageAccountKey": {
									"value": "@pipeline().parameters.StorageAccountKey",
									"type": "Expression"
								}
							}
						}
					},
					{
						"name": "Blackshark Building Detection",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Blackshark Model Transforms",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Blackshark Building Detection",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"Prefix": {
									"value": "@pipeline().parameters.Prefix",
									"type": "Expression"
								},
								"BatchName": {
									"value": "@pipeline().parameters.BatchAccountName",
									"type": "Expression"
								},
								"BatchLocation": {
									"value": "@pipeline().parameters.BatchLocation",
									"type": "Expression"
								},
								"JobName": {
									"value": "@pipeline().parameters.BatchJobName",
									"type": "Expression"
								},
								"StorageAccountName": {
									"value": "@pipeline().parameters.StorageAccountName",
									"type": "Expression"
								},
								"StorageAccountKey": {
									"value": "@pipeline().parameters.StorageAccountKey",
									"type": "Expression"
								}
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"Prefix": {
						"type": "string",
						"defaultValue": "innoblackshark"
					},
					"StorageAccountName": {
						"type": "string",
						"defaultValue": "rawdatajbqezt"
					},
					"StorageAccountKey": {
						"type": "string",
						"defaultValue": "AHOTVR7kOK3lLcJV177VGv8WaPpTd7GfYdds2XFWD7gLDD89bIzAbMj47eeLWCS+igKPwum0Ip1lCVpEUdtIrA=="
					},
					"BatchAccountName": {
						"type": "string",
						"defaultValue": "innosynorcbatchact"
					},
					"BatchJobName": {
						"type": "string",
						"defaultValue": "blackshark-model"
					},
					"BatchLocation": {
						"type": "string",
						"defaultValue": "southcentralus"
					}
				},
				"variables": {
					"StorageAccountConnString": {
						"type": "String"
					},
					"StorageAccountName": {
						"type": "String"
					}
				},
				"annotations": [],
				"lastPublishTime": "2022-05-31T20:09:08Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Blackshark Model Transforms')]",
				"[concat(variables('workspaceId'), '/pipelines/Blackshark Building Detection')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/gld')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Stellar Airbus v2",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"DestinationFolderPath": {
						"type": "string"
					},
					"DestinationContainerName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": {
							"value": "@dataset().DestinationFolderPath",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().DestinationContainerName",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Stellar Airbus v2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/gls')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Stellar Airbus v2 FS",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureFileStorageLocation"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Stellar Airbus v2 FS')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/rawtifs')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Stellar Data Storage Account v2",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"containername": {
						"type": "string"
					},
					"folderpath": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"folderPath": {
							"value": "@dataset().folderpath",
							"type": "Expression"
						},
						"container": {
							"value": "@dataset().containername",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Stellar Data Storage Account v2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/spec_doc_specification')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Stellar Data Storage Account v2",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filename": {
						"type": "string"
					},
					"folderpath": {
						"type": "string"
					},
					"containername": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": {
							"value": "@dataset().filename",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@dataset().folderpath",
							"type": "Expression"
						},
						"container": {
							"value": "@dataset().containername",
							"type": "Expression"
						}
					}
				},
				"schema": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Stellar Data Storage Account v2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LiamTest')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('LiamTest_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Stellar Airbus v2 FS')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureFileStorage",
				"typeProperties": {
					"connectionString": "[parameters('Stellar Airbus v2 FS_connectionString')]",
					"accountKey": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "Stellar Pipeline Key Vault",
							"type": "LinkedServiceReference"
						},
						"secretName": "GeospatialStorageAccountKey"
					},
					"fileShare": "volume-a"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/Stellar Pipeline Key Vault')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Stellar Airbus v2')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('Stellar Airbus v2_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Stellar Data Storage Account v2')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('Stellar Data Storage Account v2_connectionString')]",
					"accountKey": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "Stellar Pipeline Key Vault",
							"type": "LinkedServiceReference"
						},
						"secretName": "GeospatialStorageAccountKey"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/Stellar Pipeline Key Vault')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Stellar Pipeline Key Vault')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('Stellar Pipeline Key Vault_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/innoaoaidata_STAC')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Blob storage for STAC guide",
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('innoaoaidata_STAC_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/innosyn-pipeline-syn-ws-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('innosyn-pipeline-syn-ws-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/innosyn-pipeline-syn-ws-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('innosyn-pipeline-syn-ws-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ReadSpecDocumentFlow')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "spec_doc_specification",
								"type": "DatasetReference"
							},
							"name": "source"
						}
					],
					"sinks": [
						{
							"name": "sink"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'arrayOfDocuments') ~> source",
						"source sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     store: 'cache',",
						"     format: 'inline',",
						"     output: true,",
						"     saveOrder: 1) ~> sink"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/spec_doc_specification')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/STAC_PreProcessing_Guide')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Guide for documenting and sharing pre-processing steps required prior to the STAC ingestion process. This guide will use JPGs as an example",
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "Geospatialspark",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "18c0e5db-3459-43d6-a9d9-11f4e65e9117"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/251b3efb-f673-4fa5-a56e-e2f645fc33fa/resourceGroups/innosyn-pipeline-rg/providers/Microsoft.Synapse/workspaces/innosyn-pipeline-syn-ws/bigDataPools/Geospatialspark",
						"name": "Geospatialspark",
						"type": "Spark",
						"endpoint": "https://innosyn-pipeline-syn-ws.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/Geospatialspark",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# STAC Ingestion Synapse Notebook Guide\r\n",
							"This guide will demonstrate the data processing steps of rich media data into STAC. \r\n",
							"\r\n",
							"For more information on Chevron's STAC implementation - **[Chevron Atlas](https://atlas-dev.azure.chevron.com/)**\r\n",
							"\r\n",
							"This guide follows recommendations provided in CDAS - **[Data Preparation](https://cdas.azure.chevron.com/Data-Insights/Geospatial-Technology-And-Analytics/Chevron-Atlas/Getting-Started/Data-Preparation.html)**\r\n",
							"## Sections:\r\n",
							"### 1. Mount blob storages\r\n",
							"### 2. Get list of all images in source container\r\n",
							"### 3. Setting up the collection\r\n",
							"### 4. Extract file header information\r\n",
							"### 5. Create new Metadata from file header\r\n",
							"### 6. Determine if Mobile, or Aerial image (Platform)\r\n",
							"### 7. Write to destination folder w/Metadata"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### 1. Mounting rich media source and destination storage accounts\r\n",
							"Typically the source will be from the XXX and the destination will be YYYY"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"\r\n",
							"try:\r\n",
							"    mssparkutils.fs.mount(\r\n",
							"        \"wasbs://liamsource@innoaoaidata.blob.core.windows.net\",\r\n",
							"        \"/STAC_Source\",\r\n",
							"        {\"linkedService\":\"innoaoaidata_STAC\"}\r\n",
							"    )\r\n",
							"    print(\"Successful mount of source to /STAC_Source\")\r\n",
							"except:\r\n",
							"    print(\"FAILED TO MOUNT STAC SOURCE\")\r\n",
							"    print(\"Either due to the drive already been mounted, or because the path is incorrect\")\r\n",
							"\r\n",
							"\r\n",
							"try:\r\n",
							"    mssparkutils.fs.mount(\r\n",
							"        \"wasbs://liamdest@innoaoaidata.blob.core.windows.net\",\r\n",
							"        \"/STAC_Dest\",\r\n",
							"        {\"linkedService\":\"innoaoaidata_STAC\"}\r\n",
							"    )\r\n",
							"    print(\"Successful mount of source to /STAC_Dest\")\r\n",
							"except:\r\n",
							"    print(\"FAILED TO MOUNT STAC DESTINATION\")\r\n",
							"    print(\"Either due to the drive already been mounted, or because the path is incorrect\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## 2. Get list of all rich media data in source folder\r\n",
							"This function is used to list out all the "
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Return array of all files in directory of type\r\n",
							"import os\r\n",
							"\r\n",
							"def get_blob_file_list(directory, filetype):\r\n",
							"    \"\"\"\r\n",
							"    Function to get a full list of files in a directory based on file type\r\n",
							"    Arguments:\r\n",
							"        directory: Directory path of folder you want to pull files from\r\n",
							"        filetype: .XXXX being the file extension of the file you are searching for (including a dot \".\")- ex- \".JPEG\"\r\n",
							"    Returns:\r\n",
							"        Array of filepaths/filename w/extension\r\n",
							"    \"\"\"\r\n",
							"    file_array = []\r\n",
							"    for file in os.listdir(directory):\r\n",
							"        file_ext = os.path.splitext(file)[1]\r\n",
							"        if file_ext == filetype:\r\n",
							"            file_array.append(directory+file)\r\n",
							"    return file_array"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"jobId = mssparkutils.env.getJobId()\r\n",
							"img_path = f'/synfs/'+jobId+'/STAC_Source/RBU_STAC_SOURCE/'\r\n",
							"outarray = get_blob_file_list(img_path, \".JPG\")\r\n",
							"outarray"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Lets take a look at what these photos look like:"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Dispay first 5 images in blob storage\r\n",
							"import rasterio\r\n",
							"from rasterio.plot import show\r\n",
							"\r\n",
							"def view_images(directory):\r\n",
							"    \"\"\"\r\n",
							"    Function to view 5 images from a directory. Not used in the final workflow - just for show.\r\n",
							"    Arguments:\r\n",
							"        directory: Directory path of folder you want to display images of. Typically JPEG, GEOTIFF, etc ...\r\n",
							"    Returns:\r\n",
							"        Nothing\r\n",
							"    \"\"\"\r\n",
							"\r\n",
							"    count = 0\r\n",
							"    for image in get_blob_file_list(directory):\r\n",
							"        if count >=5:\r\n",
							"            src = rasterio.open(directory+image)\r\n",
							"            show(src)\r\n",
							"            src.close()\r\n",
							"            count = count + 1"
						],
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## 3. Setting up the collection\r\n",
							"\r\n",
							"We have already setup 2 collections in the destination folder titled XXX and YYY"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"jobId = mssparkutils.env.getJobId()\r\n",
							"oblique_mobile_path = f'/synfs/'+jobId+'/STAC_Dest/RBU_STAC_DEST/Mobile/Oblique/rbu_mobile_oblique_imagery_for_ehsr_reporting'\r\n",
							"oblique_aerial_path = f'/synfs/'+jobId+'/STAC_Dest/RBU_STAC_DEST/Mobile/Oblique/rbu_mobile_oblique_imagery_for_ehsr_reporting'\r\n",
							"\r\n",
							"oblique_mobile_collection_json = f'/synfs/'+jobId+'/STAC_Dest/RBU_STAC_DEST/Mobile/Oblique/rbu_mobile_oblique_imagery_for_ehsr_reporting/rbu_mobile_oblique_imagery_for_ehsr_reporting_mtl.json'\r\n",
							"oblique_aerial_collection_json = f'/synfs/'+jobId+'/STAC_Dest/RBU_STAC_DEST/Aerial/Oblique/rbu_aerial_oblique_imagery_for_ehsr_reporting/rbu_aerial_oblique_imagery_for_ehsr_reporting_mtl.json'"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Take a look at the collection MTL json files\r\n",
							"import json\r\n",
							"\r\n",
							"def check_collection_mtl(jsonpath):\r\n",
							"    \"\"\"\r\n",
							"    Print and return collection_mtl. Not used in the final workflow - just for show.\r\n",
							"    Arguments:\r\n",
							"        jsonpath: Path to json_mtl file\r\n",
							"    Returns:\r\n",
							"        json json_mtl file\r\n",
							"    \"\"\"\r\n",
							"\r\n",
							"    json_open = json.load(open(jsonpath))\r\n",
							"    print(json_open)\r\n",
							"    return json_open"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mobile_collection = check_collection_mtl(oblique_mobile_collection_json)\r\n",
							"print(\"\")\r\n",
							"aerial_collection = check_collection_mtl(oblique_aerial_collection_json)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## 4. Extract EXIF Header Information\r\n",
							"Depending on the rich media data type, this step will be different. Between LAS files, GEOTIFFs, JPEGs, etc ... this step will have to be re-written to extract metadata from those files. "
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import rasterio\r\n",
							"def extract_EXIF(jpeg_path):\r\n",
							"    \"\"\"\r\n",
							"    Function to list EXIF header information from the supplied JPEG. Not used in the final workflow - just for show.\r\n",
							"    Arguments:\r\n",
							"        jpeg_path: path to jpeg file\r\n",
							"    Returns:\r\n",
							"        Dictionary of EXIF Header information\r\n",
							"    \"\"\"\r\n",
							"\r\n",
							"    file = rasterio.open(jpeg_path)\r\n",
							"    jpeg_EXIF = file.tags()\r\n",
							"    file.close()\r\n",
							"    return jpeg_EXIF"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"extract_EXIF(f'/synfs/'+jobId+'/STAC_Source/RBU_STAC_SOURCE/202002280104265320_skilpatrick_S3_7-15-19_DJI_0256.JPG')"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## 5. Extract Metadata\r\n",
							"### 5.1 Add Bounding Box\r\n",
							"### 5.2 Add DateTime\r\n",
							"### 5.3 Add CRS\r\n",
							"### 5.4 Add Sensor Type"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#### 5.1 Bounding Box\r\n",
							"To calculate a bounding box from a JPEG you must first do 3 things:\r\n",
							"- Extract lat/long from EXIF Header (including if coordinate direction is North/South or East/West)\r\n",
							"- Convert lat/long from EXIF Header from Degree, Minutes, Seconds to Decimal Degrees\r\n",
							"- Calculate new bounding box\r\n",
							"- Return new bounding box\r\n",
							"\r\n",
							"Lastly, the data must be inserted into the \"headers\" of the JPEG. Since JPEG does not natively support adding to headers, a XML file is created via rasterio. This step will be done once we move the new images into the right folder."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import rasterio\r\n",
							"\r\n",
							"def get_EXIF_lat(jpeg_path):\r\n",
							"    \"\"\"\r\n",
							"    Function to get the Latitude of a JPEG image\r\n",
							"    Arguments:\r\n",
							"        jpeg_path: path to jpeg file\r\n",
							"    Returns:\r\n",
							"        string of latitude in format of (dd) (mm) (ss.sss) & GPS Reference in terms of (S or N)\r\n",
							"    \"\"\"\r\n",
							"    jpeg = rasterio.open(jpeg_path)\r\n",
							"    filetags = jpeg.tags()\r\n",
							"    GPSLatitude = filetags.get(\"EXIF_GPSLatitude\")\r\n",
							"    GPSLatitudeRef = filetags.get(\"EXIF_GPSLatitudeRef\")\r\n",
							"    jpeg.close()\r\n",
							"    return GPSLatitude, GPSLatitudeRef\r\n",
							"\r\n",
							"def get_EXIF_long(jpeg_path):\r\n",
							"    \"\"\"\r\n",
							"    Function to get the Longitude of a JPEG image\r\n",
							"    Arguments:\r\n",
							"        jpeg_path: path to jpeg file\r\n",
							"    Returns:\r\n",
							"        string of longitude in format of (dd) (mm) (ss.sss) & GPS Reference in terms of (W or E)\r\n",
							"    \"\"\"\r\n",
							"    jpeg = rasterio.open(jpeg_path)\r\n",
							"    filetags = jpeg.tags()\r\n",
							"    GPSLongitude = filetags.get(\"EXIF_GPSLongitude\")\r\n",
							"    GPSLongitudeRef = filetags.get(\"EXIF_GPSLongitudeRef\")\r\n",
							"\r\n",
							"    jpeg.close()\r\n",
							"    return GPSLongitude, GPSLongitudeRef"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def Exif_Cord_Convert(EXIF_Cord, direction):\r\n",
							"    \"\"\"\r\n",
							"    Function to convert coordinate from get_EXIF_long + get_EXIF_lat functions (dd)(mm)(ss.sss) to decimal degrees.\r\n",
							"    Arguments:\r\n",
							"        EXIF_Cord: return from get_EXIF_long or get_EXIF_lat (first return)\r\n",
							"        direction: return from get_EXIF_long or get_EXIF_lat (second return) - Either as \"N\",\"S\",\"E\", or \"W\"\r\n",
							"    Returns:\r\n",
							"        Decimal Degrees of coordinate.\r\n",
							"    \"\"\"\r\n",
							"\r\n",
							"    new_str = EXIF_Cord.replace(\"(\",\"\")\r\n",
							"    new_str = new_str.replace(\")\",\"\")\r\n",
							"\r\n",
							"    split_str = new_str.split(\" \")\r\n",
							"    \r\n",
							"    decimaldegrees = (float(split_str[0]) + float(split_str[1])/60 + float(split_str[2])/(60*60)) * (-1 if direction in ['W', 'S'] else 1)\r\n",
							"    return(decimaldegrees)"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pyproj\r\n",
							"from shapely.ops import transform\r\n",
							"from shapely.geometry import Point\r\n",
							"from functools import partial\r\n",
							"\r\n",
							"def calculate_bb(lat, lon):\r\n",
							"\r\n",
							"    proj_wgs84 = pyproj.Proj('+proj=longlat +datum=WGS84')\r\n",
							"    \r\n",
							"    # Azimuthal equidistant projection\r\n",
							"    aeqd_proj = '+proj=aeqd +lat_0={lat} +lon_0={lon} +x_0=0 +y_0=0'\r\n",
							"    project = partial(\r\n",
							"        pyproj.transform,\r\n",
							"        pyproj.Proj(aeqd_proj.format(lat=lat, lon=lon)),\r\n",
							"        proj_wgs84)\r\n",
							"    \r\n",
							"    buf = Point(0, 0).buffer(1)  # buffers 1 meter\r\n",
							"    return transform(project, buf).bounds"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_bb(jpeg_path):\r\n",
							"    EXIFlat, latRef = get_EXIF_lat(jpeg_path)\r\n",
							"    EXIFlong, longRef = get_EXIF_long(jpeg_path)\r\n",
							"\r\n",
							"    lat = Exif_Cord_Convert(EXIFlat,latRef)\r\n",
							"    long = Exif_Cord_Convert(EXIFlong,longRef)\r\n",
							"\r\n",
							"    return calculate_bb(lat,long)"
						],
						"outputs": [],
						"execution_count": 78
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#### 5.2 Add Date Time\r\n",
							"\r\n",
							"The STAC spec calls for a datetime to be added to the headers of the files. For this use case, EXIF information includes this data, but we have to get it into the right format. To do this, we will extract the EXIF Datetime and convert it to the STAC-ready format.\r\n",
							"\r\n",
							"In some instances, rich media is captured over a range of time (videos, stiched together aerial GEOTIFFs, and point clouds). For any of those use cases, a new function would have to be created to return a start_datetime and an end_datetime to fit to the STAC standard.\r\n",
							"\r\n",
							"This step will be done once we move the new images into the right folder."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from datetime import datetime\r\n",
							"\r\n",
							"#Output date time must be in \r\n",
							"def get_jpeg_datetime(jpeg_path):\r\n",
							"    jpeg = rasterio.open(jpeg_path)\r\n",
							"    filetags = jpeg.tags()\r\n",
							"    jpeg.close()\r\n",
							"    EXIF_Datetime = filetags.get(\"EXIF_DateTime\")\r\n",
							"    EXIF_datetime_obj = datetime.strptime(EXIF_Datetime, '%Y:%m:%d %H:%M:%S')\r\n",
							"    STAC_datetime = EXIF_datetime_obj.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\r\n",
							"    return STAC_datetime"
						],
						"outputs": [],
						"execution_count": 79
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#### 5.3 Add Coordinate Reference System\r\n",
							"The Cordinate Reference System (CRS) of a JPEG is usually taken in WGS84 (since that is the datum GPS is based on). Sometimes there will be a GPSMapDatum in the EXIF header but usually this data is not captured (much to the chagrin of geospatial analysts everywhere). For example if the image was taken in Japan, this value will be \"TOKYO\" but for this data, none is given. \r\n",
							"\r\n",
							"We will assume the data is GPS-WGS84 which has a [European Petroleum Survey Group (EPSG)](https://support.esri.com/en/technical-article/000002814#:~:text=Answer,version%20of%20the%20EPSG%20model.) standard ID of \"4326\" or otherwise captured as [EPSG:4326](https://epsg.io/4326)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_jpeg_crs(jpeg_path):\r\n",
							"    crs = \"EPSG:4326\"\r\n",
							"    return crs"
						],
						"outputs": [],
						"execution_count": 80
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#### 5.4 Add Sensor Type\r\n",
							"Determining sensor type is important when you know your datasets will contain rich media from multiple sensors. Here we define sensor as the optical capturing device's binary return. For example, this RBU imagery was captured with an RGB (Red, Green, Blue) sensor, but there are other sensors we have recognized such as Near-Infrared (NIR), Radar, and Multispectral. \r\n",
							"\r\n",
							"[Check NASA's website](https://www.earthdata.nasa.gov/sensors) for an almost exhaustive list of remote sensing sensors. There is no standard yet set for how we want the sensor formated in STAC. More to come on this.\r\n",
							"\r\n",
							"For this dataset, we will return RGB because every imagery taken, we can assume, was taken by an RGB camera."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_jpeg_sensor(jpeg_path):\r\n",
							"    sensor_type = \"RGB\"\r\n",
							"    return sensor_type"
						],
						"outputs": [],
						"execution_count": 81
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## 6. Mobile or Aerial image?\r\n",
							"\r\n",
							"Based of the proposed architecture, it is important to sort the imagery between \"Mobile\" and \"Drone\" since those are 2 different types of collections. How will we be able to do that on these images?\r\n",
							"\r\n",
							"Luckily, we know that in this use case, RBU is using \"DJI\" drones (commerical-company) which use DJI cameras. If you look in the EXIF header of the imagery you can see an attribute called \"EXIF_Make\" which contains the information about the camera maker. This item will contain:\r\n",
							"1. DJI\r\n",
							"2. Android\r\n",
							"3. Apple\r\n",
							"4. ... other mobile phones\r\n",
							"5. Blank\r\n",
							"\r\n",
							"If we test between all RBU imagery if it is \"DJI imagery\" and \"everything else\" we can then sort between \"drone\" and \"mobile\" respectively"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Return true if image was taken from drone\r\n",
							"def is_drone(jpeg_path):\r\n",
							"    jpeg = rasterio.open(jpeg_path)\r\n",
							"    filetags = jpeg.tags()\r\n",
							"    jpeg.close()\r\n",
							"    platform = filetags.get(\"EXIF_Make\")\r\n",
							"    \r\n",
							"    if platform == 'DJI':\r\n",
							"        return True\r\n",
							"    else:\r\n",
							"        return False    "
						],
						"outputs": [],
						"execution_count": 100
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## 7. Write to Collection - w/Added Metadata\r\n",
							"Finally we get into the big task of reading from the STAC Source and writing to the STAC Destination. Here are the steps we have defined:\r\n",
							"1. Get list of all JPEGs (using the function in step 2)\r\n",
							"2. Determine which JPEG fits into which collection (using function in step 6)\r\n",
							"3. Get list of all metadata components to be added (using function in step 5)\r\n",
							"4. Write to collection folder w/metadata added\r\n",
							"5. Celebrate "
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import rasterio\r\n",
							"import shutil\r\n",
							"\r\n",
							"def write_to_collection(jpeg_path, collection_path):\r\n",
							"    # BoundingBox = get_bb(jpeg_path)\r\n",
							"    # DateTime = get_jpeg_datetime(jpeg_path)\r\n",
							"    # jpeg_CRS = get_jpeg_crs(jpeg_path)\r\n",
							"    # Sensor = get_jpeg_sensor(jpeg_path)\r\n",
							"\r\n",
							"    # shutil.copy(jpeg_path, collection_path)\r\n",
							"    # print(\"Copy created \" + collection_path)\r\n",
							"\r\n",
							"    # with rasterio.open(\r\n",
							"    #         collection_path,\r\n",
							"    #         'r+',\r\n",
							"    #         driver='JPEG') as dst:\r\n",
							"    #         dst.update_tags(datetime = DateTime, start_datetime = None, end_datetime = None, bbox = BoundingBox, crs = jpeg_CRS, sensor = Sensor)\r\n",
							"\r\n",
							"    return True"
						],
						"outputs": [],
						"execution_count": 118
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import os\r\n",
							"jobId = mssparkutils.env.getJobId()\r\n",
							"img_path = f'/synfs/'+jobId+'/STAC_Source/RBU_STAC_SOURCE/'\r\n",
							"outarray = get_blob_file_list(img_path, \".JPG\")\r\n",
							"#print(outarray)\r\n",
							"oblique_mobile_path = f'/synfs/'+jobId+'/STAC_Dest/RBU_STAC_DEST/Mobile/Oblique/rbu_mobile_oblique_imagery_for_ehsr_reporting'\r\n",
							"oblique_aerial_path = f'/synfs/'+jobId+'/STAC_Dest/RBU_STAC_DEST/Aerial/Oblique/rbu_aerial_oblique_imagery_for_ehsr_reporting'\r\n",
							"\r\n",
							"for jpeg in outarray:\r\n",
							"    filename = os.path.basename(jpeg)\r\n",
							"    print(filename)\r\n",
							"    \r\n",
							"    if is_drone(jpeg):\r\n",
							"        write_to_collection(jpeg, oblique_aerial_path+'/'+filename)\r\n",
							"\r\n",
							"    else:\r\n",
							"        write_to_collection(jpeg, oblique_mobile_path+'/'+filename)\r\n",
							"\r\n",
							"    continue"
						],
						"outputs": [],
						"execution_count": 119
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/cv_pumpjack_clip')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Working document for clipping pumpjacks - trying to replicate FME job",
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "Geospatialspark",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9e25753f-5af8-417a-b8d5-e30402ca4c87"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/251b3efb-f673-4fa5-a56e-e2f645fc33fa/resourceGroups/innosyn-pipeline-rg/providers/Microsoft.Synapse/workspaces/innosyn-pipeline-syn-ws/bigDataPools/Geospatialspark",
						"name": "Geospatialspark",
						"type": "Spark",
						"endpoint": "https://innosyn-pipeline-syn-ws.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/Geospatialspark",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"#Mounting the AML blob storage located in dinnot101mlcvx to this notebook so we can reference it as if it was a filepath\r\n",
							"#The 'wasbs' is for blob storage. There is another prefix for file storage, etc ...\r\n",
							"\r\n",
							"from notebookutils import mssparkutils\r\n",
							"try:\r\n",
							"    mssparkutils.fs.mount(\r\n",
							"        \"wasbs://azureml-blobstore-91aa9420-fbec-40ef-91e9-982a277c80cf@dinnot101mlcvx.blob.core.windows.net\",\r\n",
							"        \"/testliam\",\r\n",
							"        {\"linkedService\":\"LiamTest\"}\r\n",
							"    )\r\n",
							"except:\r\n",
							"    print(\"FAILED TO MOUNT\")\r\n",
							"    print(\"Either due to the drive already been mounted, or because the path is incorrect\")"
						],
						"outputs": [],
						"execution_count": 185
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Plot jpeg image from blob storage container\r\n",
							"\r\n",
							"#This is a proof of concept of how rasterio/other python tools access files via blob storage/notebook mount.\r\n",
							"\r\n",
							"#Note 'jobId' which will change every notebook instance\r\n",
							"jobId = mssparkutils.env.getJobId()\r\n",
							"\r\n",
							"import rasterio\r\n",
							"from rasterio.plot import show\r\n",
							"img_path = f'/synfs/'+jobId+'/testliam/UI/06-13-2022_075155_UTC/2022_SJVBU_Kern_River_A_Flight_01_00217.jpg'\r\n",
							"src = rasterio.open(img_path)\r\n",
							"show(src)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#This function is not used during the cv_pumpjack_clip workflow, just wrote it for reference on which files were in a directory.\r\n",
							"import os\r\n",
							"\r\n",
							"def get_blob_file_list(directory):\r\n",
							"    for f in os.listdir(directory):\r\n",
							"        f_img = img_dir+f\r\n",
							"        print(f_img)"
						],
						"outputs": [],
						"execution_count": 186
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import json\r\n",
							"import numpy as np\r\n",
							"\r\n",
							"def parse_AML_json(json_path):\r\n",
							"    \"\"\"\r\n",
							"    Function to parse the Azure Machine Learning (AML) JSONL output. \r\n",
							"    Arguments:\r\n",
							"        json_path: The filepath to the AML jsonl file. This file should contain name, topX, topY, bottomX, and bottomY\r\n",
							"    Returns:\r\n",
							"        A numpy array of all labels from AML jsonl file in this format - [filename, topX, topyY, bottomX, bottomY]\r\n",
							"    \r\n",
							"    \"\"\"\r\n",
							"\r\n",
							"    parsed_list = np.array(['name','topX','topY','bottomX','bottomY'])\r\n",
							"\r\n",
							"    with open(json_path, 'r') as json_file:\r\n",
							"        json_list = list(json_file)\r\n",
							"        for pumpjack in json_list:\r\n",
							"            pumpjack_json = json.loads(pumpjack)\r\n",
							"\r\n",
							"            pumpjack_img = os.path.basename(pumpjack_json['image_url'])\r\n",
							"            \r\n",
							"            for pumpjack_label in pumpjack_json['label']:\r\n",
							"                topX = pumpjack_label['topX']\r\n",
							"                topY = pumpjack_label['topY']\r\n",
							"                bottomX = pumpjack_label['bottomX']\r\n",
							"                bottomY = pumpjack_label['bottomY']\r\n",
							"                \r\n",
							"                parsed_row = np.array([pumpjack_img,topX,topY,bottomX,bottomY])\r\n",
							"                #print(parsed_row)\r\n",
							"                parsed_list = np.vstack([parsed_list, parsed_row])\r\n",
							"    return parsed_list"
						],
						"outputs": [],
						"execution_count": 96
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#https://stackoverflow.com/questions/13852700/create-file-but-if-name-exists-add-number\r\n",
							"def uniquify(path):\r\n",
							"    \"\"\"\r\n",
							"    Function to return a unique path name if there is already a file that exists at path\r\n",
							"    Arguments:\r\n",
							"        path: a full directory and file name with extention\r\n",
							"    Returns:\r\n",
							"        path but with _x at the end with x being a unique number if there already exists a file with the suffix of _1, _2, _3 and so on.\r\n",
							"    \"\"\"\r\n",
							"    filename, extension = os.path.splitext(path)\r\n",
							"    counter = 1\r\n",
							"\r\n",
							"    while os.path.exists(path):\r\n",
							"        path = filename + \"_\" + str(counter) + extension\r\n",
							"        counter += 1\r\n",
							"\r\n",
							"    return path"
						],
						"outputs": [],
						"execution_count": 175
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from osgeo import gdal\r\n",
							"from os.path import exists\r\n",
							"\r\n",
							"gdal.UseExceptions()\r\n",
							"\r\n",
							"def clip_pumpjacks(pumpjack_list, img_dir, img_dir_out):\r\n",
							"    \"\"\"\r\n",
							"    Function for clipping images 1500, 1500 from Azure Machine Learning provided bonding box. Currently the read/writes of images are done via GDAL\r\n",
							"    Arguments:\r\n",
							"        pumpjack_list: list of pumpjacks returned from parse_AML_json function\r\n",
							"        img_dir: directory of where the pumpjack images to be clipped are located\r\n",
							"        img_dir_out: directory of where the clipped images will go container_tokens\r\n",
							"    Returns:\r\n",
							"        True if the function was completed.\r\n",
							"    \"\"\"\r\n",
							"\r\n",
							"    for pumpjack in pumpjack_list:\r\n",
							"        if pumpjack[0] == 'name':\r\n",
							"            continue     \r\n",
							"\r\n",
							"        img_file = img_dir + pumpjack[0]\r\n",
							"\r\n",
							"        topX = pumpjack[1]\r\n",
							"        topY = pumpjack[2]\r\n",
							"        bottomX = pumpjack[3]\r\n",
							"        bottomY = pumpjack[4]\r\n",
							"\r\n",
							"        g = gdal.OpenEx(img_file)\r\n",
							"        img_width = g.RasterXSize\r\n",
							"        img_height = g.RasterYSize\r\n",
							"\r\n",
							"        g = None\r\n",
							"\r\n",
							"        topX_pixel = int(int(img_width)*float(topX))\r\n",
							"        topY_pixel = int(int(img_height)*float(topY))\r\n",
							"        bottomX_pixel = int(int(img_width)*float(bottomX))\r\n",
							"        bottomY_pixel = int(int(img_height)*float(bottomY))\r\n",
							"\r\n",
							"        middleX_pixel = int(topX_pixel + .5*(bottomX_pixel-topX_pixel))\r\n",
							"        middleY_pixel = int(topY_pixel + .5*(bottomY_pixel-topY_pixel))\r\n",
							"\r\n",
							"        new_topX_pixel = middleX_pixel - 750\r\n",
							"        new_topY_pixel = middleY_pixel - 750\r\n",
							"\r\n",
							"        jpg_in = img_file\r\n",
							"        jpg_out_nonunique = img_dir_out + pumpjack[0] +\".jpg\"\r\n",
							"        jpg_out = uniquify(jpg_out_nonunique)\r\n",
							"        print(jpg_out)\r\n",
							"        srcWin_option = [new_topX_pixel, new_topY_pixel, 1500, 1500]\r\n",
							"        options = gdal.TranslateOptions(format='JPEG', srcWin = srcWin_option)\r\n",
							"        gdal.Translate(jpg_out, jpg_in, options=options)\r\n",
							"\r\n",
							"        # print(\"topX = \"+ str(topX_pixel))\r\n",
							"        # print(\"topY = \"+ str(topY_pixel))\r\n",
							"        # print(\"bottomX_pixel = \"+ str(bottomX_pixel))\r\n",
							"        # print(\"bottomY_pixel = \"+ str(bottomY_pixel))\r\n",
							"        # print(\" \")\r\n",
							"        # print(\"middleX_pixel = \"+ str(middleX_pixel))\r\n",
							"        # print(\"middleY_pixel = \"+ str(middleY_pixel))\r\n",
							"        # print('')\r\n",
							"        # print(str(bottomY_pixel-topY_pixel))\r\n",
							"    return True"
						],
						"outputs": [],
						"execution_count": 203
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"jobId = mssparkutils.env.getJobId()\r\n",
							"\r\n",
							"img_dir = f'/synfs/'+jobId+'/testliam/UI/06-15-2022_053557_UTC/geotagged_subset/'\r\n",
							"aml_json_path = f'/synfs/'+jobId+'/testliam/Labeling/export/dataset/b2efe480-107f-e1c2-5e78-7af2427f8f00/b5779ff4-ec96-455f-82df-92de2386cafd/labeledDatapoints_1.jsonl'\r\n",
							"img_dir_out = f'/synfs/'+jobId+'/testliam/UI/geotagged_subset_pumpjacks_clip/python_out/'\r\n",
							"\r\n",
							"#pumpjack_list = parse_AML_json(aml_json_path)\r\n",
							"\r\n",
							"#get_blob_file_list(img_dir)\r\n",
							"\r\n",
							"clip_pumpjacks(pumpjack_list, img_dir, img_dir_out)"
						],
						"outputs": [],
						"execution_count": 201
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.fs.unmount(\"/testliam\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/orbital-ai-analytics-nb')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "Geospatialspark",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9ad09075-f965-4ce7-b29a-b8ab30d70903"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/251b3efb-f673-4fa5-a56e-e2f645fc33fa/resourceGroups/innosyn-pipeline-rg/providers/Microsoft.Synapse/workspaces/innosyn-pipeline-syn-ws/bigDataPools/Geospatialspark",
						"name": "Geospatialspark",
						"type": "Spark",
						"endpoint": "https://innosyn-pipeline-syn-ws.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/Geospatialspark",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"#Print information from the raster data\r\n",
							"from osgeo import gdal  \r\n",
							"gdal.UseExceptions()\r\n",
							"access_key = TokenLibrary.getSecret('innosyn-pipeline-kv','GeospatialStorageAccountKey')\r\n",
							"gdal.SetConfigOption('AZURE_STORAGE_ACCOUNT', 'rawdatajbqezt')\r\n",
							"gdal.SetConfigOption('AZURE_STORAGE_ACCESS_KEY', access_key)  \r\n",
							"dataset_info = gdal.Info('/vsiadls/innosyn/raw/sample_4326.tif')\r\n",
							"print(dataset_info)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Convert GeoTiff to PNG by using GDAL\r\n",
							"tiff_in = \"/vsiadls/innosyn/raw/sample_4326.tif\"\r\n",
							"png_out = \"/vsiadls/innosyn/raw/sample_4326.png\"\r\n",
							"options = gdal.TranslateOptions(format='PNG')\r\n",
							"gdal.Translate(png_out, tiff_in, options=options)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Store geotif image in Azure Data Lake Storage\r\n",
							"import shutil\r\n",
							"import sys\r\n",
							"from osgeo import gdal\r\n",
							"from notebookutils import mssparkutils\r\n",
							"mssparkutils.fs.unmount('/test')\r\n",
							"gdal.UseExceptions()\r\n",
							"mssparkutils.fs.mount(\r\n",
							"    \"abfss://innosyn@rawdatajbqezt.dfs.core.windows.net\",\r\n",
							"    \"/test\",\r\n",
							"    {\"linkedService\":\"innosyn_ajvt_linked_service\"}\r\n",
							")\r\n",
							"access_key = TokenLibrary.getSecret('innosyn-pipeline-kv','GeospatialStorageAccountKey')\r\n",
							"gdal.SetConfigOption('AZURE_STORAGE_ACCOUNT', 'rawdatajbqezt')\r\n",
							"gdal.SetConfigOption('AZURE_STORAGE_ACCESS_KEY', access_key)\r\n",
							"options = gdal.WarpOptions(options=['tr'])\r\n",
							"gdal.Warp('ajvt_image.tif', '/vsiadls/innosyn/raw/sample_4326.tif', options=options)\r\n",
							"jobId = mssparkutils.env.getJobId()\r\n",
							"shutil.copy(\"ajvt_image.tif\", f\"/synfs/{jobId}/test/ajvt_image.tif\")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Plot geotif image from blob storage container\r\n",
							"import rasterio\r\n",
							"from matplotlib import pyplot\r\n",
							"file_name = \"2022_SJVBU_Kern_River_A_Flight_01_00011.JPG\"\r\n",
							"img_path = \"https://dinnot101mlcvx.blob.core.windows.net/azureml-blobstore-91aa9420-fbec-40ef-91e9-982a277c80cf/UI/05-25-2022_044402_UTC/2022_SJVBU_Kern_River_A%20Flight%2001_OUTPUT_geotagged_254/\" + file_name + \"?sv=2021-06-08&ss=bfqt&srt=co&sp=rwdlacupitfx&se=2022-06-30T21:28:32Z&st=2022-06-23T13:28:32Z&spr=https&sig=G0uIqReJKkC7C5y7UmBBR%2Fy9DwdjHRxRqgfgYi7m7ZA%3D\"\r\n",
							"src = rasterio.open(img_path)\r\n",
							"pyplot.imshow(src.read(3))\r\n",
							"pyplot.show()"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"CloudStorageAccount storageAccount1 = CloudStorageAccount.Parse(CloudConfigurationManager.GetSetting(\"ConnString\"))\r\n",
							"CloudBlobContainer container1 = blobClient.GetContainerReference(imageFolder)\r\n",
							"blobs = container1.ListBlobs().OfType<CloudBlobBlob>().OrderByDescending(b => b.Name).Where(b => b.Name.EndsWith(\".jpg\"))"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/plot-image-from-naip-nb')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "stacapipool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "febd3d25-a839-4785-aaef-49cbff6b3df8"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/251b3efb-f673-4fa5-a56e-e2f645fc33fa/resourceGroups/innosyn-pipeline-rg/providers/Microsoft.Synapse/workspaces/innosyn-pipeline-syn-ws/bigDataPools/stacapipool",
						"name": "stacapipool",
						"type": "Spark",
						"endpoint": "https://innosyn-pipeline-syn-ws.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/stacapipool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pystac_client import Client\r\n",
							"import planetary_computer as pc"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"area_of_interest = {\r\n",
							"    \"type\": \"Polygon\",\r\n",
							"    \"coordinates\": [\r\n",
							"        [\r\n",
							"            [-111.9839859008789, 40.5389819819361],\r\n",
							"            [-111.90502166748045, 40.5389819819361],\r\n",
							"            [-111.90502166748045, 40.57015381856105],\r\n",
							"            [-111.9839859008789, 40.57015381856105],\r\n",
							"            [-111.9839859008789, 40.5389819819361],\r\n",
							"        ]\r\n",
							"    ],\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"range_old = \"2010-01-01/2013-01-01\"\r\n",
							"range_new = \"2018-01-01/2021-01-01\""
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"catalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\r\n",
							"\r\n",
							"search_old = catalog.search(\r\n",
							"    collections=[\"naip\"], intersects=area_of_interest, datetime=range_old\r\n",
							")\r\n",
							"\r\n",
							"search_new = catalog.search(\r\n",
							"    collections=[\"naip\"], intersects=area_of_interest, datetime=range_new\r\n",
							")\r\n",
							"\r\n",
							"items_old = list(search_old.get_items())\r\n",
							"items_new = list(search_new.get_items())\r\n",
							"\r\n",
							"print(f\"{len(items_old)} Items found in the 'old' range\")\r\n",
							"print(f\"{len(items_new)} Items found in the 'new' range\")"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from shapely.geometry import shape\r\n",
							"\r\n",
							"area_shape = shape(area_of_interest)\r\n",
							"target_area = area_shape.area\r\n",
							"\r\n",
							"\r\n",
							"def area_of_overlap(item):\r\n",
							"    overlap_area = shape(item.geometry).intersection(shape(area_of_interest)).area\r\n",
							"    return overlap_area / target_area\r\n",
							"\r\n",
							"\r\n",
							"item_old = sorted(items_old, key=area_of_overlap, reverse=True)[0]\r\n",
							"item_new = sorted(items_new, key=area_of_overlap, reverse=True)[0]"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import rioxarray\r\n",
							"\r\n",
							"\r\n",
							"def create_image(item):\r\n",
							"    print(item.datetime)\r\n",
							"    href = pc.sign(item.assets[\"image\"].href)\r\n",
							"\r\n",
							"    ds = rioxarray.open_rasterio(href).sel(band=[4, 2, 3])\r\n",
							"    img = ds.rio.clip([area_of_interest], crs=\"epsg:4326\").plot.imshow(\r\n",
							"        rgb=\"band\", aspect=1.5, size=12\r\n",
							"    )\r\n",
							"    img.axes.set_axis_off()\r\n",
							"    return img.axes"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"create_image(item_old);"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"create_image(item_new);"
						],
						"outputs": [],
						"execution_count": 20
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/stac-planetary-computer-nb')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "stacapipool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9efa31ee-ae56-4e84-bf7d-bfce6d91ad82"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/251b3efb-f673-4fa5-a56e-e2f645fc33fa/resourceGroups/innosyn-pipeline-rg/providers/Microsoft.Synapse/workspaces/innosyn-pipeline-syn-ws/bigDataPools/stacapipool",
						"name": "stacapipool",
						"type": "Spark",
						"endpoint": "https://innosyn-pipeline-syn-ws.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/stacapipool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pystac_client import Client\r\n",
							"catalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")"
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"time_range = \"2020-12-01/2020-12-31\"\r\n",
							"bbox = [-122.2751, 47.5469, -121.9613, 47.7458]\r\n",
							"\r\n",
							"search = catalog.search(collections=[\"landsat-8-c2-l2\"], bbox=bbox, datetime=time_range)\r\n",
							"items = search.get_all_items()\r\n",
							"len(items)"
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"area_of_interest = {\r\n",
							"    \"type\": \"Polygon\",\r\n",
							"    \"coordinates\": [\r\n",
							"        [\r\n",
							"            [-122.2751, 47.5469],\r\n",
							"            [-121.9613, 47.9613],\r\n",
							"            [-121.9613, 47.9613],\r\n",
							"            [-122.2751, 47.9613],\r\n",
							"            [-122.2751, 47.5469],\r\n",
							"        ]\r\n",
							"    ],\r\n",
							"}\r\n",
							"\r\n",
							"time_range = \"2020-12-01/2020-12-31\"\r\n",
							"\r\n",
							"search = catalog.search(\r\n",
							"    collections=[\"landsat-8-c2-l2\"], intersects=area_of_interest, datetime=time_range\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import geopandas\r\n",
							"df = geopandas.GeoDataFrame.from_features(items.to_dict(), crs=\"epsg:4326\")\r\n",
							"df"
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"selected_item = min(items, key=lambda item: item.properties[\"eo:cloud_cover\"])\r\n",
							"selected_item"
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import rich.table\r\n",
							"table = rich.table.Table(\"Asset Key\", \"Descripiption\")\r\n",
							"for asset_key, asset in selected_item.assets.items():\r\n",
							"    # print(f\"{asset_key:<25} - {asset.title}\")\r\n",
							"    table.add_row(asset_key, asset.title)\r\n",
							"\r\n",
							"table"
						],
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"selected_item.assets[\"rendered_preview\"].to_dict()"
						],
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from IPython.display import Image\r\n",
							"Image(url=selected_item.assets[\"rendered_preview\"].href, width=500)"
						],
						"outputs": [],
						"execution_count": 54
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import requests\r\n",
							"url = selected_item.assets[\"SR_B2\"].href\r\n",
							"print(\"Accessing\", url)\r\n",
							"response = requests.get(url)\r\n",
							"response"
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import planetary_computer\r\n",
							"signed_href = planetary_computer.sign(selected_item).assets[\"SR_B2\"].href"
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Copy noop')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "pooldl2xmc",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.1",
				"language": "python",
				"jobProperties": {
					"name": "Copy noop",
					"file": "abfss://spark-jobs@synhnsdl2xmc.dfs.core.windows.net/copy_noop/src/mosaic.py",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "01767b3a-cede-4abf-8b79-52cb6d0ff80d"
					},
					"args": [],
					"jars": [],
					"files": [],
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Crop')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "pooldl2xmc",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.1",
				"language": "python",
				"jobProperties": {
					"name": "Crop",
					"file": "abfss://spark-jobs@synhnsdl2xmc.dfs.core.windows.net/raster_crop/src/crop.py",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f4cbbafe-9d98-476f-9bd4-e5bfc7bad06c"
					},
					"args": [],
					"jars": [],
					"files": [
						"abfss://spark-jobs@synhnsdl2xmc.dfs.core.windows.net/raster_crop/src/utils.py"
					],
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Mosaic')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "pooldl2xmc",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.1",
				"language": "python",
				"jobProperties": {
					"name": "Mosaic",
					"file": "abfss://spark-jobs@synhnsdl2xmc.dfs.core.windows.net/raster_mosaic/src/mosaic.py",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "3",
						"spark.dynamicAllocation.maxExecutors": "3",
						"spark.autotune.trackingId": "811de002-982f-4b4b-9732-147d3565c502"
					},
					"args": [],
					"jars": [],
					"files": [],
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 3
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Warp')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "pooldl2xmc",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.1",
				"language": "python",
				"jobProperties": {
					"name": "Warp",
					"file": "abfss://spark-jobs@synhnsdl2xmc.dfs.core.windows.net/raster_warp/src/warp.py",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "3",
						"spark.dynamicAllocation.maxExecutors": "3",
						"spark.autotune.trackingId": "335dd1ad-fc75-4734-ad92-03a79e9ad399"
					},
					"args": [],
					"jars": [],
					"files": [],
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 3
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pooldl2xmc')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 5,
					"minNodeCount": 1
				},
				"nodeCount": 0,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"libraryRequirements": {
					"content": "name: aoi-env\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - gdal=3.3.0\n  - pip>=20.1.1\n  - azure-storage-file-datalake\n  - libgdal\n  - shapely\n  - pyproj\n  - pip:\n    - rasterio\n    - geopandas\n",
					"filename": "/home/anthonyb/Azure-Orbital-Analytics-Samples/deploy/environment.yml",
					"time": "2022-05-10T15:53:25.0987366Z"
				},
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "southcentralus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Geospatialspark')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 8,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"libraryRequirements": {
					"content": "name: aoi-env\r\nchannels:\r\n  - defaults\r\ndependencies:\r\n  - azure-storage-file-datalake\r\n  - gdal=3.3.0\r\n  - libgdal\r\n  - pip>=20.1.1\r\n  - pyproj\r\n  - shapely\r\n  - pip:\r\n      - azure-storage-blob==0.37.1\r\n      - rasterio\r\n      - geopandas\r\n      - numpy\r\n      - opencv-python\r\n",
					"filename": "environment (2).yml",
					"time": "2022-06-27T14:26:52.5580015Z"
				},
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "southcentralus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/poolYLQS')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"libraryRequirements": {
					"content": "name: aoi-env\r\nchannels:\r\n  - defaults\r\ndependencies:\r\n  - azure-storage-file-datalake\r\n  - gdal=3.3.0\r\n  - libgdal\r\n  - pip>=20.1.1\r\n  - pyproj\r\n  - shapely\r\n  - pip:\r\n      - \"rasterio\"\r\n      - \"geopandas\"",
					"filename": "environment.yml",
					"time": "2022-06-08T16:08:46.0952706Z"
				},
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "southcentralus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/stacapipool')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"libraryRequirements": {
					"content": "name: aoi-env\r\nchannels:\r\n  - defaults\r\ndependencies:\r\n  - pystac-client  \r\n  - pip:\r\n      - rich\r\n      - planetary-computer\r\n      - rioxarray\r\n      - rasterio\r\n      - geopandas",
					"filename": "environment.yml",
					"time": "2022-06-16T20:08:12.4228171Z"
				},
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "southcentralus"
		}
	]
}