{
	"name": "cv_pumpjack_clip",
	"properties": {
		"description": "Working document for clipping pumpjacks - trying to replicate FME job",
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "Geospatialspark",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "9e25753f-5af8-417a-b8d5-e30402ca4c87"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/251b3efb-f673-4fa5-a56e-e2f645fc33fa/resourceGroups/innosyn-pipeline-rg/providers/Microsoft.Synapse/workspaces/innosyn-pipeline-syn-ws/bigDataPools/Geospatialspark",
				"name": "Geospatialspark",
				"type": "Spark",
				"endpoint": "https://innosyn-pipeline-syn-ws.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/Geospatialspark",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"#Mounting the AML blob storage located in dinnot101mlcvx to this notebook so we can reference it as if it was a filepath\r\n",
					"#The 'wasbs' is for blob storage. There is another prefix for file storage, etc ...\r\n",
					"\r\n",
					"from notebookutils import mssparkutils\r\n",
					"try:\r\n",
					"    mssparkutils.fs.mount(\r\n",
					"        \"wasbs://azureml-blobstore-91aa9420-fbec-40ef-91e9-982a277c80cf@dinnot101mlcvx.blob.core.windows.net\",\r\n",
					"        \"/testliam\",\r\n",
					"        {\"linkedService\":\"LiamTest\"}\r\n",
					"    )\r\n",
					"except:\r\n",
					"    print(\"FAILED TO MOUNT\")\r\n",
					"    print(\"Either due to the drive already been mounted, or because the path is incorrect\")"
				],
				"execution_count": 185
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Plot jpeg image from blob storage container\r\n",
					"\r\n",
					"#This is a proof of concept of how rasterio/other python tools access files via blob storage/notebook mount.\r\n",
					"\r\n",
					"#Note 'jobId' which will change every notebook instance\r\n",
					"jobId = mssparkutils.env.getJobId()\r\n",
					"\r\n",
					"import rasterio\r\n",
					"from rasterio.plot import show\r\n",
					"img_path = f'/synfs/'+jobId+'/testliam/UI/06-13-2022_075155_UTC/2022_SJVBU_Kern_River_A_Flight_01_00217.jpg'\r\n",
					"src = rasterio.open(img_path)\r\n",
					"show(src)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#This function is not used during the cv_pumpjack_clip workflow, just wrote it for reference on which files were in a directory.\r\n",
					"import os\r\n",
					"\r\n",
					"def get_blob_file_list(directory):\r\n",
					"    for f in os.listdir(directory):\r\n",
					"        f_img = img_dir+f\r\n",
					"        print(f_img)"
				],
				"execution_count": 186
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import json\r\n",
					"import numpy as np\r\n",
					"\r\n",
					"def parse_AML_json(json_path):\r\n",
					"    \"\"\"\r\n",
					"    Function to parse the Azure Machine Learning (AML) JSONL output. \r\n",
					"    Arguments:\r\n",
					"        json_path: The filepath to the AML jsonl file. This file should contain name, topX, topY, bottomX, and bottomY\r\n",
					"    Returns:\r\n",
					"        A numpy array of all labels from AML jsonl file in this format - [filename, topX, topyY, bottomX, bottomY]\r\n",
					"    \r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    parsed_list = np.array(['name','topX','topY','bottomX','bottomY'])\r\n",
					"\r\n",
					"    with open(json_path, 'r') as json_file:\r\n",
					"        json_list = list(json_file)\r\n",
					"        for pumpjack in json_list:\r\n",
					"            pumpjack_json = json.loads(pumpjack)\r\n",
					"\r\n",
					"            pumpjack_img = os.path.basename(pumpjack_json['image_url'])\r\n",
					"            \r\n",
					"            for pumpjack_label in pumpjack_json['label']:\r\n",
					"                topX = pumpjack_label['topX']\r\n",
					"                topY = pumpjack_label['topY']\r\n",
					"                bottomX = pumpjack_label['bottomX']\r\n",
					"                bottomY = pumpjack_label['bottomY']\r\n",
					"                \r\n",
					"                parsed_row = np.array([pumpjack_img,topX,topY,bottomX,bottomY])\r\n",
					"                #print(parsed_row)\r\n",
					"                parsed_list = np.vstack([parsed_list, parsed_row])\r\n",
					"    return parsed_list"
				],
				"execution_count": 96
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#https://stackoverflow.com/questions/13852700/create-file-but-if-name-exists-add-number\r\n",
					"def uniquify(path):\r\n",
					"    \"\"\"\r\n",
					"    Function to return a unique path name if there is already a file that exists at path\r\n",
					"    Arguments:\r\n",
					"        path: a full directory and file name with extention\r\n",
					"    Returns:\r\n",
					"        path but with _x at the end with x being a unique number if there already exists a file with the suffix of _1, _2, _3 and so on.\r\n",
					"    \"\"\"\r\n",
					"    filename, extension = os.path.splitext(path)\r\n",
					"    counter = 1\r\n",
					"\r\n",
					"    while os.path.exists(path):\r\n",
					"        path = filename + \"_\" + str(counter) + extension\r\n",
					"        counter += 1\r\n",
					"\r\n",
					"    return path"
				],
				"execution_count": 175
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from osgeo import gdal\r\n",
					"from os.path import exists\r\n",
					"\r\n",
					"gdal.UseExceptions()\r\n",
					"\r\n",
					"def clip_pumpjacks(pumpjack_list, img_dir, img_dir_out):\r\n",
					"    \"\"\"\r\n",
					"    Function for clipping images 1500, 1500 from Azure Machine Learning provided bonding box. Currently the read/writes of images are done via GDAL\r\n",
					"    Arguments:\r\n",
					"        pumpjack_list: list of pumpjacks returned from parse_AML_json function\r\n",
					"        img_dir: directory of where the pumpjack images to be clipped are located\r\n",
					"        img_dir_out: directory of where the clipped images will go container_tokens\r\n",
					"    Returns:\r\n",
					"        True if the function was completed.\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    for pumpjack in pumpjack_list:\r\n",
					"        if pumpjack[0] == 'name':\r\n",
					"            continue     \r\n",
					"\r\n",
					"        img_file = img_dir + pumpjack[0]\r\n",
					"\r\n",
					"        topX = pumpjack[1]\r\n",
					"        topY = pumpjack[2]\r\n",
					"        bottomX = pumpjack[3]\r\n",
					"        bottomY = pumpjack[4]\r\n",
					"\r\n",
					"        g = gdal.OpenEx(img_file)\r\n",
					"        img_width = g.RasterXSize\r\n",
					"        img_height = g.RasterYSize\r\n",
					"\r\n",
					"        g = None\r\n",
					"\r\n",
					"        topX_pixel = int(int(img_width)*float(topX))\r\n",
					"        topY_pixel = int(int(img_height)*float(topY))\r\n",
					"        bottomX_pixel = int(int(img_width)*float(bottomX))\r\n",
					"        bottomY_pixel = int(int(img_height)*float(bottomY))\r\n",
					"\r\n",
					"        middleX_pixel = int(topX_pixel + .5*(bottomX_pixel-topX_pixel))\r\n",
					"        middleY_pixel = int(topY_pixel + .5*(bottomY_pixel-topY_pixel))\r\n",
					"\r\n",
					"        new_topX_pixel = middleX_pixel - 750\r\n",
					"        new_topY_pixel = middleY_pixel - 750\r\n",
					"\r\n",
					"        jpg_in = img_file\r\n",
					"        jpg_out_nonunique = img_dir_out + pumpjack[0] +\".jpg\"\r\n",
					"        jpg_out = uniquify(jpg_out_nonunique)\r\n",
					"        print(jpg_out)\r\n",
					"        srcWin_option = [new_topX_pixel, new_topY_pixel, 1500, 1500]\r\n",
					"        options = gdal.TranslateOptions(format='JPEG', srcWin = srcWin_option)\r\n",
					"        gdal.Translate(jpg_out, jpg_in, options=options)\r\n",
					"\r\n",
					"        # print(\"topX = \"+ str(topX_pixel))\r\n",
					"        # print(\"topY = \"+ str(topY_pixel))\r\n",
					"        # print(\"bottomX_pixel = \"+ str(bottomX_pixel))\r\n",
					"        # print(\"bottomY_pixel = \"+ str(bottomY_pixel))\r\n",
					"        # print(\" \")\r\n",
					"        # print(\"middleX_pixel = \"+ str(middleX_pixel))\r\n",
					"        # print(\"middleY_pixel = \"+ str(middleY_pixel))\r\n",
					"        # print('')\r\n",
					"        # print(str(bottomY_pixel-topY_pixel))\r\n",
					"    return True"
				],
				"execution_count": 203
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"jobId = mssparkutils.env.getJobId()\r\n",
					"\r\n",
					"img_dir = f'/synfs/'+jobId+'/testliam/UI/06-15-2022_053557_UTC/geotagged_subset/'\r\n",
					"aml_json_path = f'/synfs/'+jobId+'/testliam/Labeling/export/dataset/b2efe480-107f-e1c2-5e78-7af2427f8f00/b5779ff4-ec96-455f-82df-92de2386cafd/labeledDatapoints_1.jsonl'\r\n",
					"img_dir_out = f'/synfs/'+jobId+'/testliam/UI/geotagged_subset_pumpjacks_clip/python_out/'\r\n",
					"\r\n",
					"#pumpjack_list = parse_AML_json(aml_json_path)\r\n",
					"\r\n",
					"#get_blob_file_list(img_dir)\r\n",
					"\r\n",
					"clip_pumpjacks(pumpjack_list, img_dir, img_dir_out)"
				],
				"execution_count": 201
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"mssparkutils.fs.unmount(\"/testliam\")"
				],
				"execution_count": null
			}
		]
	}
}